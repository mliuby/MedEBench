{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 6/6 [01:14<00:00, 12.43s/it]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Captured 96 cross-attn maps\n",
      "🎯 Captured 96 self-attn maps\n",
      "cross_attns[0] shape: torch.Size([2, 4096, 77])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers.models.attention import Attention\n",
    "from diffusers.models.attention_processor import AttnProcessor\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# -----------------------------------------------------------------------------\n",
    "MODEL_NAME = \"runwayml/stable-diffusion-v1-5\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "GUIDANCE_SCALE = 7.5\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# GLOBALS TO STORE ATTENTION\n",
    "# -----------------------------------------------------------------------------\n",
    "ATTENTION_LOGS = {\n",
    "    \"cross\": [],\n",
    "    \"self\": [],\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PATCH PROCESSOR TO SAVE ATTENTION\n",
    "# -----------------------------------------------------------------------------\n",
    "def make_patched_processor(base_cls):\n",
    "    class PatchedProcessor(base_cls):\n",
    "        def __call__(self, attn, hidden_states, encoder_hidden_states=None, attention_mask=None, **kwargs):\n",
    "            result = super().__call__(attn, hidden_states, encoder_hidden_states, attention_mask, **kwargs)\n",
    "\n",
    "            # Recalculate attention matrix\n",
    "            q = attn.to_q(hidden_states)\n",
    "            is_cross = encoder_hidden_states is not None\n",
    "            k = attn.to_k(encoder_hidden_states if is_cross else hidden_states)\n",
    "\n",
    "            bsz, seqlen, _ = q.shape\n",
    "            num_heads = attn.heads\n",
    "            head_dim = q.shape[-1] // num_heads\n",
    "            scale = 1 / head_dim ** 0.5\n",
    "\n",
    "            q = q.view(bsz, seqlen, num_heads, head_dim).transpose(1, 2)\n",
    "            k = k.view(bsz, -1, num_heads, head_dim).transpose(1, 2)\n",
    "\n",
    "            scores = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "            probs = torch.nn.functional.softmax(scores, dim=-1)\n",
    "            attn_map = probs.mean(1).detach().cpu()  # average over heads\n",
    "\n",
    "            if is_cross:\n",
    "                ATTENTION_LOGS[\"cross\"].append(attn_map)\n",
    "            else:\n",
    "                ATTENTION_LOGS[\"self\"].append(attn_map)\n",
    "\n",
    "            return result\n",
    "    return PatchedProcessor\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# LOAD PIPELINE\n",
    "# -----------------------------------------------------------------------------\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
    "    safety_checker=None,\n",
    ").to(DEVICE)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# REPLACE ALL PROCESSORS WITH PATCHED ONES\n",
    "# -----------------------------------------------------------------------------\n",
    "for name, module in pipe.unet.named_modules():\n",
    "    if isinstance(module, Attention):\n",
    "        cls = type(module.processor)\n",
    "        module.processor = make_patched_processor(cls)()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# RUN GENERATION\n",
    "# -----------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:00<00:37,  1.31it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:19<00:00,  2.51it/s]\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"A wisdom tooth\"\n",
    "NUM_STEPS = 50\n",
    "with torch.no_grad():\n",
    "    output = pipe(PROMPT, num_inference_steps=NUM_STEPS, guidance_scale=GUIDANCE_SCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def visualize_tokens_layers(attn_logs, image, tokenizer, prompt, layers):\n",
    "    # Get tokenized prompt\n",
    "    token_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    num_tokens = len(tokens)\n",
    "\n",
    "    fig, axs = plt.subplots(num_tokens, len(layers) + 1, figsize=(4 * (len(layers) + 1), 4 * num_tokens))\n",
    "\n",
    "    for i in range(num_tokens):\n",
    "        token = tokens[i]\n",
    "\n",
    "        for j, layer in enumerate(layers):\n",
    "            attn = attn_logs[layer][0]  # shape: [Q, K]\n",
    "            heat = attn[:, i]  # [Q] for token i\n",
    "\n",
    "            side = int(heat.shape[0] ** 0.5)\n",
    "            heatmap = heat.view(1, 1, side, side)\n",
    "            heatmap = F.interpolate(heatmap, size=(512, 512), mode=\"bilinear\", align_corners=False)[0, 0]\n",
    "            heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
    "\n",
    "            ax = axs[i][j + 1] if num_tokens > 1 else axs[j + 1]\n",
    "            ax.imshow(image)\n",
    "            ax.imshow(heatmap.numpy(), cmap=\"magma\", alpha=0.5)\n",
    "            ax.set_title(f\"Layer {layer}\")\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "        # First column = image only\n",
    "        ax_img = axs[i][0] if num_tokens > 1 else axs[0]\n",
    "        ax_img.imshow(image)\n",
    "        ax_img.set_title(f\"Token {i}: {token}\")\n",
    "        ax_img.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from diffusers import (\n",
    "    StableDiffusionInstructPix2PixPipeline,\n",
    "    EulerAncestralDiscreteScheduler,\n",
    "    DiffusionPipeline,\n",
    "    DDIMScheduler\n",
    ")\n",
    "from diffusers.models.attention import Attention\n",
    "from diffusers.models.attention_processor import AttnProcessor\n",
    "from huggingface_hub import login\n",
    "\n",
    "# === CONFIG ===\n",
    "BASE_DIR = \"../\"\n",
    "METADATA_PATH = os.path.join(BASE_DIR, \"editing_common\", \"editing_metadata.json\")\n",
    "OUTPUT_ROOT = os.path.join(BASE_DIR, \"attention_maps\")\n",
    "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "CACHE_DIR = os.path.join(BASE_DIR, \"cache\")\n",
    "HF_TOKEN = \"Your huggingface token\"\n",
    "DIFFUSION_STEPS = 50\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === GLOBAL ATTENTION STORE ===\n",
    "ATTENTION_LOGS = {\"cross\": [], \"self\": []}\n",
    "\n",
    "def make_patched_processor(base_cls):\n",
    "    class PatchedProcessor(base_cls):\n",
    "        def __call__(self, attn, hidden_states, encoder_hidden_states=None, attention_mask=None, **kwargs):\n",
    "            result = super().__call__(attn, hidden_states, encoder_hidden_states, attention_mask, **kwargs)\n",
    "            q = attn.to_q(hidden_states)\n",
    "            is_cross = encoder_hidden_states is not None\n",
    "            k = attn.to_k(encoder_hidden_states if is_cross else hidden_states)\n",
    "            bsz, seqlen, _ = q.shape\n",
    "            num_heads = attn.heads\n",
    "            head_dim = q.shape[-1] // num_heads\n",
    "            scale = 1 / head_dim ** 0.5\n",
    "            q = q.view(bsz, seqlen, num_heads, head_dim).transpose(1, 2)\n",
    "            k = k.view(bsz, -1, num_heads, head_dim).transpose(1, 2)\n",
    "            scores = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "            probs = torch.nn.functional.softmax(scores, dim=-1)\n",
    "            attn_map = probs.mean(1).detach().cpu()\n",
    "            if is_cross:\n",
    "                ATTENTION_LOGS[\"cross\"].append(attn_map)\n",
    "            else:\n",
    "                ATTENTION_LOGS[\"self\"].append(attn_map)\n",
    "            return result\n",
    "    return PatchedProcessor\n",
    "\n",
    "def patch_attention(pipe):\n",
    "    for name, module in pipe.unet.named_modules():\n",
    "        if isinstance(module, Attention):\n",
    "            module.processor = make_patched_processor(type(module.processor))()\n",
    "\n",
    "# === PIPELINE LOADER ===\n",
    "def load_ip2p_pipeline(model_id):\n",
    "    pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16 if DEVICE.type == \"cuda\" else torch.float32,\n",
    "        safety_checker=None\n",
    "    )\n",
    "    pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "    patch_attention(pipe)\n",
    "    return pipe.to(DEVICE)\n",
    "\n",
    "# === GENERATION ===\n",
    "def generate_with_ip2p(pipe, sample, output_dir, seeds=1, igs_values=[1.6], gs_values=[7.5], constant_seed=False):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    img_id = sample[\"id\"]\n",
    "    prompt = sample[\"prompt\"]\n",
    "    # img_path = os.path.normpath(os.path.join(BASE_DIR, sample[\"previous_image\"]))\n",
    "    img_path = os.path.join(BASE_DIR, sample[\"previous_image\"].replace(\"\\\\\", \"/\"))\n",
    "\n",
    "    if not os.path.exists(img_path):\n",
    "        print(f\"Image not found for ID {img_id}: {img_path}\")\n",
    "        return\n",
    "\n",
    "    original_image = Image.open(img_path).convert(\"RGB\")\n",
    "    image = original_image.resize((512, 512))\n",
    "    seed_list = [0] * seeds if constant_seed else [random.randint(0, 9999) for _ in range(seeds)]\n",
    "    idx = 0\n",
    "\n",
    "    for seed in seed_list:\n",
    "        for igs in igs_values:\n",
    "            for gs in gs_values:\n",
    "                generator = torch.manual_seed(seed)\n",
    "                result = pipe(\n",
    "                    prompt,\n",
    "                    image=image,\n",
    "                    num_inference_steps=DIFFUSION_STEPS,\n",
    "                    image_guidance_scale=igs,\n",
    "                    guidance_scale=gs,\n",
    "                    generator=generator\n",
    "                )\n",
    "                result_image = result.images[0]\n",
    "                result_path = os.path.join(output_dir, f\"{img_id}_{idx}.png\")\n",
    "                result_image.save(result_path)\n",
    "\n",
    "                # === ATTENTION VISUALIZATION ===\n",
    "                tokens = pipe.tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "                decoded = pipe.tokenizer.convert_ids_to_tokens(tokens)\n",
    "                token_ids_to_show = list(range(len(decoded)))\n",
    "\n",
    "                TARGET_SHAPE = (64, 64)  # Resize all attention maps to this\n",
    "\n",
    "                for tid in token_ids_to_show:\n",
    "                    token = decoded[tid]\n",
    "                    maps = []\n",
    "                    for attn in ATTENTION_LOGS[\"cross\"]:\n",
    "                        token_map = attn[0, :, tid]  # [Q]\n",
    "                        Q = token_map.shape[0]\n",
    "                        H = W = int(Q ** 0.5)\n",
    "                        while H * W != Q and H > 1:\n",
    "                            H -= 1\n",
    "                            W = Q // H if Q % H == 0 else W\n",
    "                        if H * W != Q:\n",
    "                            continue\n",
    "                        heat = token_map.view(1, 1, H, W)\n",
    "                        heat = F.interpolate(heat, size=TARGET_SHAPE, mode=\"bilinear\", align_corners=False)\n",
    "                        maps.append(heat)\n",
    "\n",
    "                    if len(maps) > 0:\n",
    "                        avg_map = torch.stack(maps).mean(dim=0)[0, 0]  # [64, 64]\n",
    "                        avg_map = (avg_map - avg_map.min()) / (avg_map.max() - avg_map.min())\n",
    "                        heatmap = F.interpolate(avg_map.unsqueeze(0).unsqueeze(0), size=(512, 512), mode=\"bilinear\")[0, 0]\n",
    "\n",
    "                        fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "                        im0 = axs[0].imshow(heatmap.numpy(), cmap=\"magma\")\n",
    "                        axs[0].set_title(f\"Attention Map\\nToken: '{token}'\")\n",
    "                        axs[0].axis(\"off\")\n",
    "                        fig.colorbar(im0, ax=axs[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "                        axs[1].imshow(original_image.resize((512, 512)))\n",
    "                        axs[1].imshow(heatmap.numpy(), cmap=\"magma\", alpha=0.5)\n",
    "                        axs[1].set_title(f\"Original + Attention\\nToken: '{token}'\")\n",
    "                        axs[1].axis(\"off\")\n",
    "\n",
    "                        axs[2].imshow(result_image)\n",
    "                        axs[2].imshow(heatmap.numpy(), cmap=\"magma\", alpha=0.5)\n",
    "                        axs[2].set_title(f\"Edited + Attention\\nToken: '{token}'\")\n",
    "                        axs[2].axis(\"off\")\n",
    "\n",
    "                        plt.tight_layout()\n",
    "                        plt.savefig(os.path.join(output_dir, f\"{img_id}_{idx}_avg_attn_token{tid}.png\"))\n",
    "                        plt.close()\n",
    "\n",
    "                ATTENTION_LOGS[\"cross\"].clear()\n",
    "                ATTENTION_LOGS[\"self\"].clear()\n",
    "                idx += 1\n",
    "\n",
    "# === PROCESSING ===\n",
    "def process_model(model_id, model_name, sample_slice=None):\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    with open(METADATA_PATH, \"r\") as f:\n",
    "        samples = json.load(f)[\"samples\"]\n",
    "    if sample_slice:\n",
    "        samples = samples[sample_slice]\n",
    "    output_dir = os.path.join(OUTPUT_ROOT, model_name)\n",
    "    pipe = load_ip2p_pipeline(model_id)\n",
    "    for sample in tqdm(samples, desc=f\"Editing with {model_name}\"):\n",
    "        generate_with_ip2p(pipe, sample, output_dir)\n",
    "\n",
    "# === MAIN ===\n",
    "def main():\n",
    "    login(HF_TOKEN)\n",
    "    process_model(\n",
    "        model_id=\"timbrooks/instruct-pix2pix\",\n",
    "        model_name=\"instruct-pix2pix_common\",\n",
    "        sample_slice=slice(None, 3)\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ip2p",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
