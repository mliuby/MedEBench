{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step1: Sample 20 images for each organ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Path to your metadata file\n",
    "metadata_path = \"../../editing/editing_metadata.json\"\n",
    "output_path = \"sampled_ids_by_organ.json\"\n",
    "\n",
    "# Load metadata\n",
    "with open(metadata_path, \"r\") as f:\n",
    "    data = json.load(f)[\"samples\"]\n",
    "\n",
    "# Group IDs by Organ\n",
    "organ_to_ids = defaultdict(list)\n",
    "for sample in data:\n",
    "    organ = sample.get(\"Organ\")\n",
    "    sample_id = sample.get(\"id\")\n",
    "    if organ and sample_id is not None:\n",
    "        organ_to_ids[organ].append(sample_id)\n",
    "\n",
    "# Sample 10 IDs per organ\n",
    "sampled = {\n",
    "    organ: random.sample(ids, min(20, len(ids)))\n",
    "    for organ, ids in organ_to_ids.items()\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(sampled, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Sampled IDs saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step2: Human evaluation by ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "from PIL import Image, ImageTk\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Configuration ===\n",
    "base_dir = os.path.abspath(\"../../\")\n",
    "metadata_path = os.path.join(base_dir, \"editing/editing_metadata.json\")\n",
    "sampled_ids_path = \"sampled_ids_by_organ.json\"\n",
    "results_path = \"human_rank.json\"\n",
    "\n",
    "model_dirs = {\n",
    "    \"gemini_2_flash\": os.path.join(base_dir, \"generated_images/gemini_2_flash\"),\n",
    "    \"seedx\": os.path.join(base_dir, \"generated_images/seedx\"),\n",
    "    \"imagic-sd-v1-4\": os.path.join(base_dir, \"generated_images/imagic-sd-v1-4\"),\n",
    "    \"instruct-pix2pix\": os.path.join(base_dir, \"generated_images/instruct-pix2pix\"),\n",
    "    \"instruct-diffusion\": os.path.join(base_dir, \"generated_images/instruct-diffusion\"),\n",
    "    \"paint-by-inpaint\": os.path.join(base_dir, \"generated_images/paint-by-inpaint\"),\n",
    "    \"icedit\": os.path.join(base_dir, \"generated_images/icedit\")\n",
    "}\n",
    "\n",
    "with open(metadata_path, \"r\") as f:\n",
    "    metadata = {str(s[\"id\"]): s for s in json.load(f)[\"samples\"]}\n",
    "\n",
    "with open(sampled_ids_path, \"r\") as f:\n",
    "    organ_to_ids = json.load(f)\n",
    "\n",
    "# Load previous progress if exists\n",
    "if os.path.exists(results_path):\n",
    "    with open(results_path, \"r\") as f:\n",
    "        results = json.load(f)\n",
    "else:\n",
    "    results = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))\n",
    "\n",
    "model_names = list(model_dirs.keys())\n",
    "\n",
    "def load_and_resize(path, size):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    img = img.resize(size, Image.BILINEAR)\n",
    "    return ImageTk.PhotoImage(img)\n",
    "\n",
    "def create_panel(prompt, prev_path, changed_path, sid, callback):\n",
    "    root = tk.Tk()\n",
    "    root.title(f\"Rank the Models for ID {sid}\")\n",
    "\n",
    "    tk.Label(root, text=prompt, font=(\"Arial\", 14), wraplength=1000).pack()\n",
    "\n",
    "    size = (128, 128)\n",
    "    imgs = []\n",
    "    for p in [prev_path, changed_path]:\n",
    "        if not os.path.exists(p):\n",
    "            messagebox.showerror(\"Error\", f\"Image not found: {p}\")\n",
    "            root.destroy()\n",
    "            callback(\"next\")\n",
    "            return\n",
    "        imgs.append(load_and_resize(p, size))\n",
    "\n",
    "    frame1 = tk.Frame(root)\n",
    "    tk.Label(frame1, text=\"Previous\").pack(side=\"left\")\n",
    "    tk.Label(frame1, image=imgs[0]).pack(side=\"left\")\n",
    "    tk.Label(frame1, text=\"Changed\").pack(side=\"left\")\n",
    "    tk.Label(frame1, image=imgs[1]).pack(side=\"left\")\n",
    "    frame1.pack()\n",
    "\n",
    "    edited_images = []\n",
    "    for model in model_names:\n",
    "        path = os.path.join(model_dirs[model], f\"{sid}_0.png\")\n",
    "        if os.path.exists(path):\n",
    "            edited_images.append((model, load_and_resize(path, size)))\n",
    "        else:\n",
    "            edited_images.append((model, None))\n",
    "\n",
    "    frame2 = tk.Frame(root)\n",
    "    entry_boxes = {\"accuracy\": {}, \"context\": {}, \"quality\": {}}\n",
    "\n",
    "    for idx, (model, img) in enumerate(edited_images):\n",
    "        col = tk.Frame(frame2)\n",
    "        if img:\n",
    "            tk.Label(col, image=img).pack()\n",
    "        else:\n",
    "            tk.Label(col, text=\"Missing\", width=20, height=10, bg=\"gray\").pack()\n",
    "\n",
    "        for metric in entry_boxes:\n",
    "            label_text = {\n",
    "                \"accuracy\": \"Editing Accuracy (0-6)\",\n",
    "                \"context\": \"Contextual Preservation (0-6)\",\n",
    "                \"quality\": \"Visual Quality (0-6)\"\n",
    "            }[metric]\n",
    "            tk.Label(col, text=label_text, font=(\"Arial\", 8)).pack()\n",
    "            entry = tk.Entry(col, width=5)\n",
    "            entry.pack()\n",
    "            entry_boxes[metric][model] = entry\n",
    "\n",
    "        col.pack(side=\"left\", padx=5)\n",
    "    frame2.pack()\n",
    "\n",
    "    def submit_and_exit():\n",
    "        save_results()\n",
    "        print(\"üëã Exiting and saving progress.\")\n",
    "        root.update()  # ensure all events are processed\n",
    "        root.destroy()\n",
    "        os._exit(0)  # force quit in Jupyter environment\n",
    "\n",
    "    def submit():\n",
    "        try:\n",
    "            scores = {metric: {} for metric in entry_boxes}\n",
    "            for metric in entry_boxes:\n",
    "                for model, entry in entry_boxes[metric].items():\n",
    "                    val = entry.get().strip()\n",
    "                    if val not in map(str, range(7)):\n",
    "                        raise ValueError(f\"Invalid score {val} for {metric} - must be 0 to 6\")\n",
    "                    scores[metric][model] = int(val)\n",
    "            root.destroy()\n",
    "            callback(scores)\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", str(e))\n",
    "\n",
    "    button_frame = tk.Frame(root)\n",
    "    tk.Button(button_frame, text=\"‚úî Submit\", command=submit).pack(side=\"left\", padx=10)\n",
    "    tk.Button(button_frame, text=\"üíæ Save & Quit\", command=submit_and_exit).pack(side=\"left\", padx=10)\n",
    "    tk.Button(button_frame, text=\"‚è≠ Skip\", command=lambda: [root.destroy(), callback(\"next\")]).pack(side=\"left\", padx=10)\n",
    "    button_frame.pack(pady=10)\n",
    "\n",
    "    root.mainloop()\n",
    "\n",
    "def save_results():\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"‚úÖ Progress saved to {results_path}\")\n",
    "\n",
    "def run_full_ranking():\n",
    "    for organ, ids in organ_to_ids.items():\n",
    "        for sid in tqdm(ids, desc=f\"Ranking {organ}\"):\n",
    "            sid_str = str(sid)\n",
    "            if sid_str in results.get(organ, {}):\n",
    "                continue\n",
    "\n",
    "            sample = metadata[sid_str]\n",
    "            prompt = sample[\"prompt\"]\n",
    "            prev_path = os.path.join(base_dir, sample[\"previous_image\"])\n",
    "            changed_path = os.path.join(base_dir, sample[\"changed_image\"])\n",
    "\n",
    "            finished = False\n",
    "            while not finished:\n",
    "                def store(scores):\n",
    "                    nonlocal finished\n",
    "                    if scores == \"next\":\n",
    "                        finished = True\n",
    "                    elif scores == \"quit\":\n",
    "                        save_results()\n",
    "                        print(\"üëã Exiting...\")\n",
    "                        exit(0)\n",
    "                    else:\n",
    "                        for metric in scores:\n",
    "                            results.setdefault(organ, {}).setdefault(sid_str, {})[metric] = scores[metric]\n",
    "                        save_results()\n",
    "                        finished = True\n",
    "\n",
    "                create_panel(prompt, prev_path, changed_path, sid_str, store)\n",
    "\n",
    "    save_results()\n",
    "    print(\"\\n‚úÖ Completed full ranking.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_full_ranking()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "from PIL import Image, ImageTk\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Configuration ===\n",
    "base_dir = os.path.abspath(\"../../\")\n",
    "metadata_path = os.path.join(base_dir, \"editing/editing_metadata.json\")\n",
    "sampled_ids_path = \"sampled_ids_by_organ.json\"\n",
    "results_path = os.path.join(base_dir, \"evaluation_result/human_ranked_scores.json\")\n",
    "automated_metrics_path = os.path.join(base_dir, \"evaluation_result/automated_metrics_ranked_scores.json\")\n",
    "human_prev_scores_path = os.path.join(base_dir, \"evaluation_result/human_ranked_scores_prev.json\")\n",
    "\n",
    "model_dirs = {\n",
    "    \"gemini_2_flash\": os.path.join(base_dir, \"generated_images/gemini_2_flash\"),\n",
    "    \"seedx\": os.path.join(base_dir, \"generated_images/seedx\"),\n",
    "    \"imagic-sd-v1-4\": os.path.join(base_dir, \"generated_images/imagic-sd-v1-4\"),\n",
    "    \"instruct-pix2pix\": os.path.join(base_dir, \"generated_images/instruct-pix2pix\"),\n",
    "    \"instruct-diffusion\": os.path.join(base_dir, \"generated_images/instruct-diffusion\"),\n",
    "    \"paint-by-inpaint\": os.path.join(base_dir, \"generated_images/paint-by-inpaint\"),\n",
    "    \"icedit\": os.path.join(base_dir, \"generated_images/icedit\")\n",
    "}\n",
    "\n",
    "# Load metadata and scores\n",
    "with open(metadata_path, \"r\") as f:\n",
    "    metadata = {str(s[\"id\"]): s for s in json.load(f)[\"samples\"]}\n",
    "\n",
    "with open(sampled_ids_path, \"r\") as f:\n",
    "    organ_to_ids = json.load(f)\n",
    "\n",
    "with open(automated_metrics_path, \"r\") as f:\n",
    "    automated_metrics = json.load(f)\n",
    "\n",
    "if os.path.exists(human_prev_scores_path):\n",
    "    with open(human_prev_scores_path, \"r\") as f:\n",
    "        human_prev_scores = json.load(f)\n",
    "else:\n",
    "    human_prev_scores = {}\n",
    "\n",
    "# Load previous progress if exists\n",
    "if os.path.exists(results_path):\n",
    "    with open(results_path, \"r\") as f:\n",
    "        results = json.load(f)\n",
    "else:\n",
    "    results = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))\n",
    "\n",
    "model_names = list(model_dirs.keys())\n",
    "\n",
    "def load_and_resize(path, size):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    img = img.resize(size, Image.BILINEAR)\n",
    "    return ImageTk.PhotoImage(img)\n",
    "\n",
    "def create_panel(prompt, prev_path, changed_path, sid, organ, callback):\n",
    "    root = tk.Tk()\n",
    "    root.title(f\"Rank the Models for ID {sid}\")\n",
    "\n",
    "    tk.Label(root, text=prompt, font=(\"Arial\", 14), wraplength=1000).pack()\n",
    "\n",
    "    size = (128, 128)\n",
    "    imgs = []\n",
    "    for p in [prev_path, changed_path]:\n",
    "        if not os.path.exists(p):\n",
    "            messagebox.showerror(\"Error\", f\"Image not found: {p}\")\n",
    "            root.destroy()\n",
    "            callback(\"next\")\n",
    "            return\n",
    "        imgs.append(load_and_resize(p, size))\n",
    "\n",
    "    frame1 = tk.Frame(root)\n",
    "    tk.Label(frame1, text=\"Previous\").pack(side=\"left\")\n",
    "    tk.Label(frame1, image=imgs[0]).pack(side=\"left\")\n",
    "    tk.Label(frame1, text=\"Changed\").pack(side=\"left\")\n",
    "    tk.Label(frame1, image=imgs[1]).pack(side=\"left\")\n",
    "    frame1.pack()\n",
    "\n",
    "    edited_images = []\n",
    "    for model in model_names:\n",
    "        path = os.path.join(model_dirs[model], f\"{sid}_0.png\")\n",
    "        if os.path.exists(path):\n",
    "            edited_images.append((model, load_and_resize(path, size)))\n",
    "        else:\n",
    "            edited_images.append((model, None))\n",
    "\n",
    "    frame2 = tk.Frame(root)\n",
    "    entry_boxes = {\"accuracy\": {}, \"context\": {}, \"quality\": {}}\n",
    "    default_fields = {\n",
    "        \"accuracy\": \"gpt4o_editing_accuracy_detailed\",\n",
    "        \"context\": \"masked_ssim\",\n",
    "        \"quality\": \"gpt4o_visual_quality_detailed\"\n",
    "    }\n",
    "\n",
    "    for idx, (model, img) in enumerate(edited_images):\n",
    "        col = tk.Frame(frame2)\n",
    "        if img:\n",
    "            tk.Label(col, image=img).pack()\n",
    "        else:\n",
    "            tk.Label(col, text=\"Missing\", width=20, height=10, bg=\"gray\").pack()\n",
    "\n",
    "        for metric in entry_boxes:\n",
    "            label_text = {\n",
    "                \"accuracy\": \"Editing Accuracy (0-6)\",\n",
    "                \"context\": \"Contextual Preservation (0-6)\",\n",
    "                \"quality\": \"Visual Quality (0-6)\"\n",
    "            }[metric]\n",
    "            tk.Label(col, text=label_text, font=(\"Arial\", 8)).pack()\n",
    "            entry = tk.Entry(col, width=5)\n",
    "\n",
    "            # Try human prev scores first, then fallback to automated\n",
    "            default_value = human_prev_scores.get(organ, {}).get(sid, {}).get(metric, {}).get(model)\n",
    "            if default_value is None:\n",
    "                default_key = default_fields[metric]\n",
    "                default_value = automated_metrics.get(organ, {}).get(sid, {}).get(default_key, {}).get(model)\n",
    "\n",
    "            if default_value is not None:\n",
    "                entry.insert(0, str(default_value))\n",
    "\n",
    "            entry.pack()\n",
    "            entry_boxes[metric][model] = entry\n",
    "\n",
    "        col.pack(side=\"left\", padx=5)\n",
    "    frame2.pack()\n",
    "\n",
    "    def submit_and_exit():\n",
    "        save_results()\n",
    "        print(\"üëã Exiting and saving progress.\")\n",
    "        root.update()\n",
    "        root.destroy()\n",
    "        os._exit(0)\n",
    "\n",
    "    def submit():\n",
    "        try:\n",
    "            scores = {metric: {} for metric in entry_boxes}\n",
    "            for metric in entry_boxes:\n",
    "                for model, entry in entry_boxes[metric].items():\n",
    "                    val = entry.get().strip()\n",
    "                    if val not in map(str, range(7)):\n",
    "                        raise ValueError(f\"Invalid score {val} for {metric} - must be 0 to 6\")\n",
    "                    scores[metric][model] = int(val)\n",
    "            root.destroy()\n",
    "            callback(scores)\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", str(e))\n",
    "\n",
    "    button_frame = tk.Frame(root)\n",
    "    tk.Button(button_frame, text=\"‚úî Submit\", command=submit).pack(side=\"left\", padx=10)\n",
    "    tk.Button(button_frame, text=\"üíæ Save & Quit\", command=submit_and_exit).pack(side=\"left\", padx=10)\n",
    "    tk.Button(button_frame, text=\"‚è≠ Skip\", command=lambda: [root.destroy(), callback(\"next\")]).pack(side=\"left\", padx=10)\n",
    "    button_frame.pack(pady=10)\n",
    "\n",
    "    root.mainloop()\n",
    "\n",
    "def save_results():\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"‚úÖ Progress saved to {results_path}\")\n",
    "\n",
    "def run_full_ranking():\n",
    "    for organ, ids in organ_to_ids.items():\n",
    "        for sid in tqdm(ids, desc=f\"Ranking {organ}\"):\n",
    "            sid_str = str(sid)\n",
    "            if sid_str in results.get(organ, {}):\n",
    "                continue\n",
    "\n",
    "            sample = metadata[sid_str]\n",
    "            prompt = sample[\"prompt\"]\n",
    "            prev_path = os.path.join(base_dir, sample[\"previous_image\"])\n",
    "            changed_path = os.path.join(base_dir, sample[\"changed_image\"])\n",
    "\n",
    "            finished = False\n",
    "            while not finished:\n",
    "                def store(scores):\n",
    "                    nonlocal finished\n",
    "                    if scores == \"next\":\n",
    "                        finished = True\n",
    "                    elif scores == \"quit\":\n",
    "                        save_results()\n",
    "                        print(\"üëã Exiting...\")\n",
    "                        exit(0)\n",
    "                    else:\n",
    "                        for metric in scores:\n",
    "                            results.setdefault(organ, {}).setdefault(sid_str, {})[metric] = scores[metric]\n",
    "                        save_results()\n",
    "                        finished = True\n",
    "\n",
    "                create_panel(prompt, prev_path, changed_path, sid_str, organ, store)\n",
    "\n",
    "    save_results()\n",
    "    print(\"\\n‚úÖ Completed full ranking.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_full_ranking()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step3: Automated metrics detailed scores to ranked scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved ranked scores to: ../../evaluation_result/gpt4o_ranked_scores.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# === Configuration ===\n",
    "metric_file = \"../../evaluation_result/gpt4o_detailed_scores.json\"\n",
    "sample_file = \"sampled_ids_by_organ.json\"\n",
    "output_file = \"../../evaluation_result/gpt4o_ranked_scores.json\"\n",
    "\n",
    "\n",
    "# === Load input JSONs ===\n",
    "with open(metric_file, \"r\") as f:\n",
    "    metric_data = json.load(f)\n",
    "\n",
    "with open(sample_file, \"r\") as f:\n",
    "    sampled_ids = json.load(f)\n",
    "\n",
    "metrics = list(metric_data[\"gemini_2_flash\"][\"1\"].keys())\n",
    "# === Format into nested dict: {organ: {sample_id: {metric: {model: rank}}}} ===\n",
    "nested_result = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))\n",
    "\n",
    "# === Process each metric ===\n",
    "for target_metric in metrics:\n",
    "    ranking_records = []\n",
    "\n",
    "    for organ, ids in sampled_ids.items():\n",
    "        for sid in ids:\n",
    "            sid = str(sid)\n",
    "            rows = []\n",
    "            for model, samples in metric_data.items():\n",
    "                if sid in samples and target_metric in samples[sid]:\n",
    "                    rows.append({\n",
    "                        \"organ\": organ,\n",
    "                        \"sample_id\": sid,\n",
    "                        \"model\": model,\n",
    "                        \"score\": samples[sid][target_metric]\n",
    "                    })\n",
    "\n",
    "            random.shuffle(rows)  # shuffle to randomize rank for tied scores\n",
    "            df = pd.DataFrame(rows)\n",
    "            df[\"rank_score\"] = (\n",
    "                df[\"score\"].rank(method=\"first\", ascending=False) - 1\n",
    "            ).astype(int)\n",
    "            df[\"rank_score\"] = len(rows)-1 - df[\"rank_score\"]  # adjust so highest = 6, lowest = 0\n",
    "\n",
    "            for _, row in df.iterrows():\n",
    "                nested_result[row[\"organ\"]][row[\"sample_id\"]][target_metric][row[\"model\"]] = int(row[\"rank_score\"])\n",
    "\n",
    "# === Save to file ===\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(nested_result, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved ranked scores to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step4: Calculate Correlation score between human evaluation and automated evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# === Configuration ===\n",
    "metrics = ['imagereward', 'imagereward2', 'clip_prompt_score', 'clip_prompt_score_2', \n",
    " 'clip_image_image_score', 'editclip', 'editclip_2', 'medclip_prompt_score', \n",
    " 'medclip_prompt_score_2', 'medclip_image_image_score', 'medclip_editclip', \n",
    "'medclip_editclip_2', 'psnr', 'lpips', 'masked_ssim', \"gpt4o_editing_accuracy_prompt\",\n",
    "\"gpt4o_contextual_preservation_prompt\", \"gpt4o_visual_quality_prompt\", \"gpt4o_editing_accuracy_detailed\",\n",
    "\"gpt4o_contextual_preservation_detailed\", \"gpt4o_visual_quality_detailed\"]\n",
    "\n",
    "human_metrics = [\"accuracy\", \"context\", \"quality\"]\n",
    "\n",
    "human_path = \"../../evaluation_result/human_ranked_scores.json\"\n",
    "metric_path = \"../../evaluation_result/automated_metrics_ranked_scores.json\"\n",
    "\n",
    "# === Load JSONs ===\n",
    "with open(human_path, \"r\") as f:\n",
    "    human_eval = json.load(f)\n",
    "\n",
    "with open(metric_path, \"r\") as f:\n",
    "    metric_eval = json.load(f)\n",
    "\n",
    "# === Aggregate function across all organs\n",
    "def average_scores_all_organs(data, metric):\n",
    "    model_totals = {}\n",
    "    model_counts = {}\n",
    "    for organ in data:\n",
    "        for sid, metrics in data[organ].items():\n",
    "            for model, score in metrics.get(metric, {}).items():\n",
    "                model_totals[model] = model_totals.get(model, 0) + score\n",
    "                model_counts[model] = model_counts.get(model, 0) + 1\n",
    "    return {model: model_totals[model] / model_counts[model] for model in model_totals}\n",
    "\n",
    "# === Loop over each metric and compare to human evaluation\n",
    "for human_metric in human_metrics:\n",
    "    for metric_metric in metrics:\n",
    "        human_score = average_scores_all_organs(human_eval, human_metric)\n",
    "        metric_score = average_scores_all_organs(metric_eval, metric_metric)\n",
    "\n",
    "        common_models = set(human_score.keys()) & set(metric_score.keys())\n",
    "        if len(common_models) < 2:\n",
    "            continue  # not enough data to compare\n",
    "\n",
    "        x = [human_score[m] for m in common_models]\n",
    "        y = [metric_score[m] for m in common_models]\n",
    "        labels = list(common_models)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.regplot(x=x, y=y, ci=None, scatter_kws={\"s\": 80})\n",
    "        for i, label in enumerate(labels):\n",
    "            plt.text(x[i] + 0.01, y[i], label, fontsize=10)\n",
    "\n",
    "        corr, _ = pearsonr(x, y)\n",
    "        plt.title(f\"Correlation between {human_metric} and {metric_metric} (All Organs)\\nPearson r = {corr:.2f}\")\n",
    "        plt.xlabel(f\"Avg {human_metric} Score (Human Ranking)\")\n",
    "        plt.ylabel(f\"Avg {metric_metric} Score (Automated Ranking)\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load files\n",
    "with open(\"../../evaluation_result/human_ranked_scores_prev.json\", \"r\") as f:\n",
    "    human_data = json.load(f)\n",
    "\n",
    "with open(\"../../evaluation_result/automated_metrics_ranked_scores.json\", \"r\") as f:\n",
    "    auto_data = json.load(f)\n",
    "\n",
    "# Detect metric names\n",
    "human_metrics = set()\n",
    "auto_metrics = set()\n",
    "\n",
    "for organ in human_data:\n",
    "    for sid in human_data[organ]:\n",
    "        human_metrics.update(human_data[organ][sid].keys())\n",
    "\n",
    "for organ in auto_data:\n",
    "    for sid in auto_data[organ]:\n",
    "        auto_metrics.update(auto_data[organ][sid].keys())\n",
    "\n",
    "human_metrics = sorted(human_metrics)\n",
    "auto_metrics = sorted(auto_metrics)\n",
    "\n",
    "# Convert scores to ranks (default: higher score = better rank)\n",
    "def score_to_rank(score_dict, reverse=True):\n",
    "    sorted_models = sorted(score_dict.items(), key=lambda x: -x[1] if reverse else x[1])\n",
    "    return {model: rank + 1 for rank, (model, _) in enumerate(sorted_models)}\n",
    "\n",
    "# Compute Spearman correlation for rank vectors\n",
    "def compute_spearman_rho(rh, rm):\n",
    "    n = len(rh)\n",
    "    m = len(rh[0])\n",
    "    if n < 2:\n",
    "        return None\n",
    "    models = rh[0].keys()\n",
    "    rhos = []\n",
    "    for i in range(n):\n",
    "        diff = 0\n",
    "        for model in models:\n",
    "            diff += (rh[i][model] - rm[i][model]) ** 2\n",
    "        rho = 1 - (6 * diff) / (m * (m**2 - 1))\n",
    "        rhos.append(rho)\n",
    "    return round(sum(rhos) / n, 4)\n",
    "\n",
    "# === GLOBAL CORRELATION TABLE ===\n",
    "global_results = []\n",
    "\n",
    "for h_metric in human_metrics:\n",
    "    row = {}\n",
    "    for a_metric in auto_metrics:\n",
    "        all_rh = []\n",
    "        all_rm = []\n",
    "        for organ in human_data:\n",
    "            for sid in human_data[organ]:\n",
    "                h_scores = human_data[organ][sid][h_metric]\n",
    "                a_scores = auto_data[organ][sid][a_metric]\n",
    "\n",
    "                reverse_order = False if a_metric == \"lpips\" else True\n",
    "                rh_rank = score_to_rank(h_scores)\n",
    "                rm_rank = score_to_rank(a_scores, reverse=reverse_order)\n",
    "\n",
    "                all_rh.append(rh_rank)\n",
    "                all_rm.append(rm_rank)\n",
    "        rho = compute_spearman_rho(all_rh, all_rm)\n",
    "        row[a_metric] = rho\n",
    "    global_results.append(row)\n",
    "\n",
    "df = pd.DataFrame(global_results, index=human_metrics)\n",
    "df.index.name = \"Human Metric\"\n",
    "df.columns.name = \"Automated Metric\"\n",
    "\n",
    "# === PER ORGAN CORRELATION TABLES ===\n",
    "per_organ_dfs = {}\n",
    "\n",
    "for organ in human_data:\n",
    "    organ_results = []\n",
    "    for h_metric in human_metrics:\n",
    "        row = {}\n",
    "        for a_metric in auto_metrics:\n",
    "            all_rh = []\n",
    "            all_rm = []\n",
    "            for sid in human_data[organ]:\n",
    "                h_scores = human_data[organ][sid][h_metric]\n",
    "                a_scores = auto_data[organ][sid][a_metric]\n",
    "\n",
    "                reverse_order = False if a_metric == \"lpips\" else True\n",
    "                rh_rank = score_to_rank(h_scores)\n",
    "                rm_rank = score_to_rank(a_scores, reverse=reverse_order)\n",
    "\n",
    "                all_rh.append(rh_rank)\n",
    "                all_rm.append(rm_rank)\n",
    "            rho = compute_spearman_rho(all_rh, all_rm)\n",
    "            row[a_metric] = rho\n",
    "        organ_results.append(row)\n",
    "    organ_df = pd.DataFrame(organ_results, index=human_metrics)\n",
    "    organ_df.index.name = \"Human Metric\"\n",
    "    organ_df.columns.name = \"Automated Metric\"\n",
    "    per_organ_dfs[organ] = organ_df\n",
    "\n",
    "# === Display with Highlights ===\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "print(\"\\nüìä Global Spearman Rank Correlation Table:\\n\")\n",
    "display(df.style.highlight_max(axis=1, color='lightgreen'))\n",
    "output_dir = \"../../evaluation_result/spearman_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "# Save global table to CSV\n",
    "df.to_csv(os.path.join(output_dir, \"spearman_global.csv\"))\n",
    "print(\"‚úÖ Global correlation saved to 'spearman_global.csv'\")\n",
    "\n",
    "# Display and save each per-organ table\n",
    "for organ, organ_df in per_organ_dfs.items():\n",
    "    print(f\"\\nüìä Spearman Correlation for Organ: {organ}\\n\")\n",
    "    display(organ_df.style.highlight_max(axis=1, color='lightyellow'))\n",
    "\n",
    "    # Save to CSV\n",
    "    filename = f\"spearman_{organ.lower()}.csv\"\n",
    "    organ_df.to_csv(os.path.join(output_dir,filename))\n",
    "    print(f\"‚úÖ Saved: {filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ip2p",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
