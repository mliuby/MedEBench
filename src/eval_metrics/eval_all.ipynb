{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from openai import OpenAI\n",
    "\n",
    "# ========== Configuration ==========\n",
    "base_dir = os.path.abspath(\"../../\")\n",
    "EDIT_META_PATH = \"editing/editing_metadata.json\"\n",
    "EVAL_OUTPUT_PATH = \"evaluation_result/gpt4o_detailed_scores.json\"\n",
    "API_KEY = \"Your OpenAI API Key\"\n",
    "MAX_WORKERS = 4\n",
    "\n",
    "# client = OpenAI(api_key=API_KEY)\n",
    "client = OpenAI(\n",
    "    api_key = API_KEY\n",
    ")\n",
    "image_cache = {}\n",
    "\n",
    "# ========== Image Encoding ==========\n",
    "def encode_image_base64_cached(image_path):\n",
    "    if image_path not in image_cache:\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            image_cache[image_path] = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    return image_cache[image_path]\n",
    "\n",
    "# ========== Prompt Builder ==========\n",
    "def build_gpt4o_prompt(editing_prompt, prev_img, edited_img, gt_img):\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": f\"I have an image editing task. Here's the editing prompt:\\n\\\"{editing_prompt}\\\"\"}\n",
    "        ]},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Here is the input image:\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{prev_img}\"}}\n",
    "        ]},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Here is the groundtruth image you can refer to:\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{gt_img}\"}}\n",
    "        ]},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Here is the edited image:\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{edited_img}\"}}\n",
    "        ]},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": (\n",
    "                \"You are a good medical image analysis expert. Evaluate the edit using the following steps:\\n\\n\"\n",
    "                \"**Step 1: Visual Difference Description**\\n\"\n",
    "                \"Compare the input and edited images. Describe all visible differences between them, including:\\n\"\n",
    "                \"- Additions, removals, or modifications of visual elements.\\n\"\n",
    "                \"- Emphasize the extent of the change and specify which anatomical regions were affected.\\n\"\n",
    "                \"List the differences clearly and item by item.\\n\\n\"\n",
    "                \"**Step 2: Evaluation (Three Scores)**\\n\"\n",
    "                \"**1. Editing Accuracy (0-10):**\\n\"\n",
    "                \"- Score strictly based on alignment with the editing prompt. Deduct points for any inaccuracy or missing elements.\\n\\n\"\n",
    "                \"**2. Contextual Preservation (0-10):**\\n\"\n",
    "                \"- Were parts of the image outside the editing prompt left untouched?\\n\"\n",
    "                \"- Were there unintended or unnecessary changes to unrelated anatomy or background?\\n\\n\"\n",
    "                \"**3. Visual Quality (0-10):**\\n\"\n",
    "                \"- Consider clarity, sharpness, blur, artifacts, naturalness, and consistency.\\n\\n\"\n",
    "                \"Please format your evaluation as follows:\\n\"\n",
    "                \"- Editing Accuracy: [Score]/10\\n\"\n",
    "                \"- Contextual Preservation: [Score]/10\\n\"\n",
    "                \"- Visual Quality: [Score]/10\"\n",
    "            )}\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "# ========== Response Parser ==========\n",
    "def parse_response(response_text):\n",
    "    scores = {\"editing_accuracy\": -1.0, \"contextual_preservation\": -1.0, \"visual_quality\": -1.0}\n",
    "    for line in response_text.split(\"\\n\"):\n",
    "        try:\n",
    "            if \"Editing Accuracy\" in line:\n",
    "                scores[\"editing_accuracy\"] = float(line.split(\":\")[1].split(\"/\")[0].strip())\n",
    "            elif \"Contextual Preservation\" in line:\n",
    "                scores[\"contextual_preservation\"] = float(line.split(\":\")[1].split(\"/\")[0].strip())\n",
    "            elif \"Visual Quality\" in line:\n",
    "                scores[\"visual_quality\"] = float(line.split(\":\")[1].split(\"/\")[0].strip())\n",
    "        except Exception:\n",
    "            continue\n",
    "    return scores\n",
    "\n",
    "# ========== GPT-4o Evaluation ==========\n",
    "def evaluate_sample(prompt, prev_path, edit_path, gt_path):\n",
    "    prev_b64 = encode_image_base64_cached(prev_path)\n",
    "    edit_b64 = encode_image_base64_cached(edit_path)\n",
    "    gt_b64 = encode_image_base64_cached(gt_path)\n",
    "    messages = build_gpt4o_prompt(prompt, prev_b64, edit_b64, gt_b64)\n",
    "\n",
    "    for attempt in range(1):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=messages,\n",
    "                max_tokens=400\n",
    "            )\n",
    "            content = response.choices[0].message.content\n",
    "            scores = parse_response(content)\n",
    "        except Exception as e:\n",
    "            print(f\"[Attempt {attempt+1}] API error: {e}\")\n",
    "            scores = None\n",
    "\n",
    "        if scores and all(score != -1.0 for score in scores.values()):\n",
    "            return scores\n",
    "        else:\n",
    "            print(f\"[Attempt {attempt+1}] Failed or incomplete scores. Retrying...\")\n",
    "            time.sleep(1)\n",
    "\n",
    "    return {\n",
    "        \"editing_accuracy\": -1,\n",
    "        \"contextual_preservation\": -1,\n",
    "        \"visual_quality\": -1\n",
    "    }\n",
    "\n",
    "# ========== Main Evaluation Process ==========\n",
    "def evaluate_all_models(model_dirs, metadata_path, output_path, test_mode=False, sample_limit=5, use_multithread=True):\n",
    "    with open(metadata_path) as f:\n",
    "        metadata_full = {str(s[\"id\"]): s for s in json.load(f)[\"samples\"]}\n",
    "\n",
    "    if test_mode:\n",
    "        metadata = dict(list(metadata_full.items())[:sample_limit])\n",
    "    else:\n",
    "        metadata = metadata_full\n",
    "\n",
    "    # Load previous results if resuming\n",
    "    if os.path.exists(output_path):\n",
    "        with open(output_path) as f:\n",
    "            all_results = json.load(f)\n",
    "    else:\n",
    "        all_results = {}\n",
    "\n",
    "    for model_name, model_dir in model_dirs.items():\n",
    "        print(f\"\\nEvaluating: {model_name}\")\n",
    "        model_result = {}\n",
    "\n",
    "        def process_sample(sid, sample):\n",
    "            edited_path = os.path.join(model_dir, f\"{sid}_0.png\")\n",
    "            if not os.path.exists(edited_path):\n",
    "                return sid, None\n",
    "            detailed_prompt_scores = evaluate_sample(\n",
    "                sample[\"detailed_prompt\"],\n",
    "                os.path.join(base_dir, sample[\"previous_image\"]),\n",
    "                edited_path,\n",
    "                os.path.join(base_dir, sample[\"changed_image\"]),\n",
    "            )\n",
    "            return sid, {\n",
    "                \"gpt4o_editing_accuracy_detailed\": detailed_prompt_scores[\"editing_accuracy\"],\n",
    "                \"gpt4o_contextual_preservation_detailed\": detailed_prompt_scores[\"contextual_preservation\"],\n",
    "                \"gpt4o_visual_quality_detailed\": detailed_prompt_scores[\"visual_quality\"]\n",
    "            }\n",
    "\n",
    "        existing_model_result = all_results.get(model_name, {})\n",
    "        sids_to_process = [sid for sid in metadata if str(sid) not in existing_model_result]\n",
    "\n",
    "        if use_multithread:\n",
    "            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "                futures = {\n",
    "                    executor.submit(process_sample, sid, metadata[sid]): sid\n",
    "                    for sid in sids_to_process\n",
    "                }\n",
    "                for future in tqdm(as_completed(futures), total=len(futures), desc=f\"{model_name}\"):\n",
    "                    sid, result = future.result()\n",
    "                    if result:\n",
    "                        model_result[sid] = result\n",
    "                all_results.setdefault(model_name, {}).update(model_result)\n",
    "                with open(output_path, \"w\") as f:\n",
    "                    json.dump(all_results, f, indent=2)\n",
    "        else:\n",
    "            for sid in tqdm(sids_to_process, desc=f\"{model_name}\"):\n",
    "                sid, result = process_sample(sid, metadata[sid])\n",
    "                if result:\n",
    "                    model_result[sid] = result\n",
    "            all_results.setdefault(model_name, {}).update(model_result)\n",
    "            with open(output_path, \"w\") as f:\n",
    "                json.dump(all_results, f, indent=2)\n",
    "    print(f\"\\nâœ… Results saved to {output_path}\")\n",
    "    \n",
    "# ========== Run ==========\n",
    "if __name__ == \"__main__\":\n",
    "    model_dirs = {\n",
    "        \"gemini_2_flash\": os.path.join(base_dir, \"generated_images/gemini_2_flash\"),\n",
    "        \"seedx\": os.path.join(base_dir, \"generated_images/seedx\"),\n",
    "        \"imagic-sd-v1-4\": os.path.join(base_dir, \"generated_images/imagic-sd-v1-4\"),\n",
    "        \"instruct-pix2pix\": os.path.join(base_dir, \"generated_images/instruct-pix2pix\"),\n",
    "        \"instruct-diffusion\": os.path.join(base_dir, \"generated_images/instruct-diffusion\"),\n",
    "        \"paint-by-inpaint\": os.path.join(base_dir, \"generated_images/paint-by-inpaint\"),\n",
    "        \"icedit\": os.path.join(base_dir, \"generated_images/icedit\")\n",
    "    }\n",
    "    \n",
    "    evaluate_all_models(\n",
    "        model_dirs=model_dirs,\n",
    "        metadata_path=os.path.join(base_dir, EDIT_META_PATH),\n",
    "        output_path=os.path.join(base_dir, EVAL_OUTPUT_PATH),\n",
    "        test_mode=False,\n",
    "        sample_limit=5,\n",
    "        use_multithread=True  # Set to False to disable threading\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_evn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
